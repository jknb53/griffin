```markdown
# ğŸš€ Project Griffin | Griffin é¡¹ç›®

*Bilingual README: Jump to [English](#-english) | [ä¸­æ–‡](#-ä¸­æ–‡)*

## ğŸ“‹ Phase I Status: COMPLETED âœ…

**Phase I "Full-Stack Operator Forging" has been successfully completed!** All core deep learning operators have been implemented from scratch with both CPU and CUDA versions, achieving full functional parity and correctness validation.

---

### ğŸ‡¬ğŸ‡§ English

A lightweight deep learning inference framework built from scratch in C++ and CUDA. This project is a hands-on implementation following the principles of Andrej Karpathy's "Neural Networks: Zero to Hero" course, with a focus on understanding the first principles of modern deep learning models like GPT-2.

**ğŸ‰ Phase I Achievement**: Complete implementation of fundamental deep learning operators with rigorous CPU-GPU validation, modular architecture, and comprehensive error analysis documentation.

#### ğŸ¯ Project Goals

The primary objective of this project is not to build a production-ready library, but to serve as a rigorous, practice-based learning journey. The key goals are:

-   **Deepen Understanding of Transformers:** Deconstruct high-level concepts like Self-Attention into their fundamental mathematical operators and implement them from scratch.
-   **Master C++/CUDA Programming:** Gain proficiency in C++/CUDA for high-performance computing, including memory management, kernel programming, and orchestrating complex computation flows on the GPU.
-   **Embrace Professional Engineering Practices:** Adhere to a strict workflow using modern CMake for building, Git for version control (following a feature-branching model), and a TDD-like approach by verifying every CUDA implementation against a "golden standard" CPU version.

#### âœ¨ Phase I Implementation Status

**All core components have been successfully implemented, tested, and documented:** âœ…

-   **Core Data Structure:**
    -   [x] A simple `Tensor` struct in C++ for handling multi-dimensional data.

-   **CPU Operators (`cpu_ops.cpp`):** âœ… **PHASE I COMPLETE**
    -   [x] `matmul_cpu`: Naive matrix multiplication serving as the "golden standard".
    -   [x] `softmax_cpu`: Numerically stable softmax, applied row-wise.
    -   [x] `self_attention_cpu`: Complete, verifiable CPU implementation of the self-attention mechanism.
    -   [x] `layernorm_cpu`: Layer normalization with configurable gamma and beta parameters.
    -   [x] `gelu_cpu`: Gaussian Error Linear Unit activation function.
    -   [x] `add_bias_cpu`: Bias addition operation with broadcasting support.
    -   [x] `ffn_cpu`: Complete FeedForward Network implementation.

-   **GPU Operators (`kernel.cu`):** âœ… **PHASE I COMPLETE**
    -   [x] `matmul_cuda`: Pure-GPU matrix multiplication with custom kernel implementation.
    -   [x] `self_attention_cuda_v2`: High-performance, pure-GPU self-attention pipeline eliminating CPU-GPU roundtrips.
        -   Includes optimized kernels: `scale_kernel`, `softmax_kernel` with parallel reduction.
    -   [x] `layernorm_cuda`: GPU-accelerated layer normalization with shared memory optimization.
    -   [x] `gelu_cuda`: GPU GELU activation with element-wise parallelization.
    -   [x] `add_bias_cuda`: GPU bias addition with efficient block-stride memory access patterns.
    -   [x] `ffn_cuda`: Complete GPU pipeline orchestrating multi-kernel computation flow.

-   **Project Architecture & Engineering:** âœ… **PHASE I COMPLETE**
    -   [x] Modular codebase structure with separation of concerns (`cpu_ops.cpp`, `kernel.cu`, `utils.h`).
    -   [x] Robust CMake build system handling C++/CUDA mixed compilation.
    -   [x] Comprehensive testing framework with CPU-GPU numerical validation.
    -   [x] Professional Git workflow with feature branching and incremental development.

-   **Documentation & Knowledge Synthesis:** âœ… **PHASE I COMPLETE**
    -   [x] Complete technical review documentation (`playbook/phase1/`).
    -   [x] Error analysis and debugging methodology documentation.
    -   [x] Performance optimization insights and parallel computing pattern analysis.

#### ğŸ› ï¸ How to Build and Run

##### Prerequisites

-   A C++ compiler (g++)
-   NVIDIA CUDA Toolkit (nvcc)
-   CMake (version 3.10+)

##### Build Steps

Clone the repository and run the following commands from the project root directory:

```bash
mkdir build
cd build
cmake ..
make
```

##### Run Tests

The main executable runs a comprehensive test to verify the correctness of the FeedForward Network implementation, which includes all core operators working in sequence.

```bash
./griffin
```

A `[SUCCESS]` message indicates that the CPU and GPU implementations of the complete FFN pipeline produce matching results, validating the correctness of:

- Matrix multiplication (`matmul`)
- Bias addition (`add_bias`)
- GELU activation function (`gelu`)
- The complete FeedForward Network orchestration (`ffn`)

#### ğŸ—ï¸ Phase I Architecture Highlights

**Rigorous Development Philosophy:**

- **"CPU Defines Truth"**: Every GPU implementation is validated against its CPU counterpart using strict numerical comparison.
- **Incremental Complexity**: Starting with individual operators and building up to complete neural network components.
- **Memory Management Mastery**: Explicit CUDA memory management demonstrates deep understanding of GPU computing principles.
- **Error-Driven Learning**: Comprehensive debugging sessions documented for future reference and learning.

**Key Technical Achievements:**

- **Pure GPU Pipelines**: The `ffn_cuda` implementation orchestrates an entire computation graph on GPU without CPU-GPU round trips.
- **Parallel Computing Mastery**: Implementation of multiple parallel patterns:
  - 2D parallel (matrix multiplication)
  - 1D reduction parallel (LayerNorm with shared memory)
  - 1D element-wise parallel (GELU activation)
- **System Integration Excellence**: Successfully orchestrated multi-kernel workflows with proper memory lifecycle management.
- **Modular Architecture**: Clean separation between CPU reference implementations and GPU optimizations.

**Engineering Lessons Learned:**

- **"Debugging First Law"**: The most critical errors often hide in the most unexpected places.
- **Memory Hierarchy Optimization**: Understanding register vs shared memory vs global memory access patterns.
- **Asynchronous Execution Awareness**: Proper synchronization between CPU and GPU execution contexts.

#### ğŸ—ºï¸ Development Roadmap

**âœ… Phase I: "Full-Stack Operator Forging" - COMPLETED**
- [x] Core tensor data structure and CPU reference implementations
- [x] GPU kernel implementations for all fundamental operators  
- [x] Complete FeedForward Network with multi-kernel orchestration
- [x] Comprehensive testing and validation framework
- [x] Modular architecture and professional engineering practices
- [x] Technical documentation and knowledge synthesis

**ğŸš§ Phase II: "High-Performance System Integration" - UPCOMING**
- [ ] Advanced GPU memory optimization techniques
- [ ] Kernel fusion and computation graph optimization  
- [ ] Multi-Head Attention mechanism implementation
- [ ] Complete GPT-2 Transformer Block assembly
- [ ] Performance profiling and bottleneck analysis

**ğŸ”® Phase III: "Production-Ready Inference Engine" - PLANNED**
- [ ] Model loading from pre-trained checkpoints (GPT-2)
- [ ] Text tokenization and preprocessing pipeline
- [ ] Batched inference optimization
- [ ] End-to-end text generation capabilities
- [ ] Performance benchmarking against production frameworks

---

### ğŸ‡¨ğŸ‡³ ä¸­æ–‡

ä¸€ä¸ªä»é›¶å¼€å§‹ï¼Œä½¿ç”¨C++å’ŒCUDAæ„å»ºçš„è½»é‡çº§æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶ã€‚æœ¬é¡¹ç›®æ˜¯å¯¹ Andrej Karpathy çš„ "Neural Networks: Zero to Hero" è¯¾ç¨‹ç†å¿µçš„äº²æ‰‹å®è·µï¼Œä¸“æ³¨äºä»ç¬¬ä¸€æ€§åŸç†ç†è§£å¦‚GPT-2ç­‰ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº•å±‚è¿ä½œæœºåˆ¶ã€‚

**ğŸ‰ ç¬¬ä¸€é˜¶æ®µæˆæœ**: å®Œæˆäº†æ‰€æœ‰åŸºç¡€æ·±åº¦å­¦ä¹ ç®—å­çš„ä»é›¶å®ç°ï¼Œå»ºç«‹äº†ä¸¥æ ¼çš„CPU-GPUéªŒè¯ä½“ç³»ï¼Œå®ç°äº†æ¨¡å—åŒ–æ¶æ„ï¼Œå¹¶å®Œæˆäº†å…¨é¢çš„é”™è¯¯åˆ†æä¸æŠ€æœ¯æ€»ç»“æ–‡æ¡£ã€‚

#### ğŸ¯ é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®çš„æ ¸å¿ƒå¹¶éæ„å»ºä¸€ä¸ªç”Ÿäº§çº§çš„ä»£ç åº“ï¼Œè€Œæ˜¯ä¸€æ¬¡ä¸¥æ ¼çš„ã€åŸºäºå®è·µçš„åˆ»æ„ç»ƒä¹ ä¹‹æ—…ã€‚ä¸»è¦ç›®æ ‡åŒ…æ‹¬ï¼š

- **æ·±åŒ–å¯¹Transformerçš„ç†è§£ï¼š** å°†è‡ªæ³¨æ„åŠ› (Self-Attention) ç­‰é«˜çº§æ¦‚å¿µï¼Œæ‹†è§£ä¸ºå…¶æœ€åŸºç¡€çš„æ•°å­¦ç®—å­ï¼Œå¹¶ä»é›¶å¼€å§‹å®ç°å®ƒä»¬ã€‚
- **æŒæ¡C++/CUDAç¼–ç¨‹ï¼š** ç†Ÿç»ƒè¿ç”¨C++/CUDAè¿›è¡Œé«˜æ€§èƒ½è®¡ç®—ï¼ŒåŒ…æ‹¬å†…å­˜ç®¡ç†ã€æ ¸å‡½æ•°ç¼–ç¨‹ä»¥åŠåœ¨GPUä¸Šç¼–æ’å¤æ‚çš„è®¡ç®—æµã€‚
- **æ‹¥æŠ±ä¸“ä¸šå·¥ç¨‹å®è·µï¼š** éµå¾ªä¸¥è°¨çš„å·¥ä½œæµï¼Œä½¿ç”¨ç°ä»£CMakeæ„å»ºé¡¹ç›®ï¼Œé€šè¿‡Gitè¿›è¡Œç‰ˆæœ¬æ§åˆ¶ï¼ˆéµå¾ªåŠŸèƒ½åˆ†æ”¯æ¨¡å‹ï¼‰ï¼Œå¹¶é‡‡ç”¨ç±»ä¼¼TDDçš„æ–¹æ³•ï¼Œå°†æ¯ä¸€ä¸ªCUDAå®ç°ä¸â€œé»„é‡‘æ ‡å‡†â€çš„CPUç‰ˆæœ¬è¿›è¡Œæ­£ç¡®æ€§éªŒè¯ã€‚

#### âœ¨ ç¬¬ä¸€é˜¶æ®µå®ç°çŠ¶æ€

**æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å·²æˆåŠŸå®ç°ã€æµ‹è¯•å¹¶æ–‡æ¡£åŒ–:** âœ…

- **æ ¸å¿ƒæ•°æ®ç»“æ„:**
  - [x] ç”¨äºå¤„ç†å¤šç»´æ•°æ®çš„ç®€æ˜“ `Tensor` C++ ç»“æ„ä½“ã€‚

- **CPU ç®—å­ (`cpu_ops.cpp`):** âœ… **ç¬¬ä¸€é˜¶æ®µå®Œæˆ**
  - [x] `matmul_cpu`: ä½œä¸º"é»„é‡‘æ ‡å‡†"çš„æœ´ç´ çŸ©é˜µä¹˜æ³•å®ç°ã€‚
  - [x] `softmax_cpu`: é€è¡Œåº”ç”¨çš„ã€æ•°å€¼ç¨³å®šçš„CPU Softmaxã€‚
  - [x] `self_attention_cpu`: å®Œæ•´çš„ã€å¯ä½œä¸ºåŸºå‡†éªŒè¯çš„CPUè‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°ã€‚
  - [x] `layernorm_cpu`: å¸¦æœ‰å¯é…ç½®gammaå’Œbetaå‚æ•°çš„å±‚å½’ä¸€åŒ–ã€‚
  - [x] `gelu_cpu`: é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒæ¿€æ´»å‡½æ•°ã€‚
  - [x] `add_bias_cpu`: æ”¯æŒå¹¿æ’­æœºåˆ¶çš„åç½®åŠ æ³•æ“ä½œã€‚
  - [x] `ffn_cpu`: å®Œæ•´çš„å‰é¦ˆç¥ç»ç½‘ç»œå®ç°ã€‚

- **GPU ç®—å­ (`kernel.cu`):** âœ… **ç¬¬ä¸€é˜¶æ®µå®Œæˆ**
  - [x] `matmul_cuda`: çº¯GPUå®ç°çš„çŸ©é˜µä¹˜æ³•ï¼ŒåŒ…å«è‡ªå®šä¹‰æ ¸å‡½æ•°å®ç°ã€‚
  - [x] `self_attention_cuda_v2`: é«˜æ€§èƒ½çº¯GPUè‡ªæ³¨æ„åŠ›æµæ°´çº¿ï¼Œæ¶ˆé™¤CPU-GPUå¾€è¿”ä¼ è¾“ã€‚
    - åŒ…å«ä¼˜åŒ–çš„æ ¸å‡½æ•°ï¼š`scale_kernel`ã€å¸¦å¹¶è¡Œè§„çº¦çš„ `softmax_kernel`ã€‚
  - [x] `layernorm_cuda`: å¸¦å…±äº«å†…å­˜ä¼˜åŒ–çš„GPUåŠ é€Ÿå±‚å½’ä¸€åŒ–ã€‚
  - [x] `gelu_cuda`: é€å…ƒç´ å¹¶è¡ŒåŒ–çš„GPU GELUæ¿€æ´»å‡½æ•°ã€‚
  - [x] `add_bias_cuda`: é‡‡ç”¨é«˜æ•ˆå—æ­¥é•¿å†…å­˜è®¿é—®æ¨¡å¼çš„GPUåç½®åŠ æ³•ã€‚
  - [x] `ffn_cuda`: ç¼–æ’å¤šæ ¸å‡½æ•°è®¡ç®—æµçš„å®Œæ•´GPUæµæ°´çº¿ã€‚

- **é¡¹ç›®æ¶æ„ä¸å·¥ç¨‹å®è·µ:** âœ… **ç¬¬ä¸€é˜¶æ®µå®Œæˆ**
  - [x] å…³æ³¨ç‚¹åˆ†ç¦»çš„æ¨¡å—åŒ–ä»£ç ç»“æ„ (`cpu_ops.cpp`, `kernel.cu`, `utils.h`)ã€‚
  - [x] å¤„ç†C++/CUDAæ··åˆç¼–è¯‘çš„å¼ºå¥CMakeæ„å»ºç³»ç»Ÿã€‚
  - [x] å¸¦CPU-GPUæ•°å€¼éªŒè¯çš„å…¨é¢æµ‹è¯•æ¡†æ¶ã€‚
  - [x] é‡‡ç”¨åŠŸèƒ½åˆ†æ”¯å’Œå¢é‡å¼€å‘çš„ä¸“ä¸šGitå·¥ä½œæµã€‚

- **æ–‡æ¡£åŒ–ä¸çŸ¥è¯†æ€»ç»“:** âœ… **ç¬¬ä¸€é˜¶æ®µå®Œæˆ**
  - [x] å®Œæ•´çš„æŠ€æœ¯å¤ç›˜æ–‡æ¡£ (`playbook/phase1/`)ã€‚
  - [x] é”™è¯¯åˆ†æä¸è°ƒè¯•æ–¹æ³•è®ºæ–‡æ¡£ã€‚
  - [x] æ€§èƒ½ä¼˜åŒ–æ´å¯Ÿä¸å¹¶è¡Œè®¡ç®—æ¨¡å¼åˆ†æã€‚

#### ğŸ› ï¸ å¦‚ä½•æ„å»ºä¸è¿è¡Œ

##### ç¯å¢ƒè¦æ±‚

- C++ ç¼–è¯‘å™¨ (g++)
- NVIDIA CUDA Toolkit (nvcc)
- CMake (3.10+ç‰ˆæœ¬)

##### æ„å»ºæ­¥éª¤

å…‹éš†æœ¬ä»“åº“ï¼Œå¹¶åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
mkdir build
cd build
cmake ..
make
```

##### è¿è¡Œæµ‹è¯•

ç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶å°†è¿è¡Œä¸€ä¸ªç»¼åˆæµ‹è¯•ï¼ŒéªŒè¯å‰é¦ˆç¥ç»ç½‘ç»œå®ç°çš„æ­£ç¡®æ€§ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰æ ¸å¿ƒç®—å­æŒ‰é¡ºåºå·¥ä½œçš„æƒ…å†µã€‚

```bash
./griffin
```

è¾“å‡º `[SUCCESS]` ä¿¡æ¯ä»£è¡¨CPUå’ŒGPUçš„å®Œæ•´FFNæµæ°´çº¿å®ç°ç»“æœä¸€è‡´ï¼ŒéªŒè¯äº†ä»¥ä¸‹ç»„ä»¶çš„æ­£ç¡®æ€§ï¼š

- çŸ©é˜µä¹˜æ³• (`matmul`)
- åç½®åŠ æ³• (`add_bias`)
- GELUæ¿€æ´»å‡½æ•° (`gelu`)
- å®Œæ•´å‰é¦ˆç¥ç»ç½‘ç»œç¼–æ’ (`ffn`)

#### ğŸ—ï¸ ç¬¬ä¸€é˜¶æ®µæ¶æ„äº®ç‚¹

**ä¸¥è°¨çš„å¼€å‘ç†å¿µï¼š**

- **"CPUå®šä¹‰çœŸç†"**ï¼šæ¯ä¸ªGPUå®ç°éƒ½é€šè¿‡ä¸å…¶CPUå¯¹åº”ç‰ˆæœ¬è¿›è¡Œä¸¥æ ¼çš„æ•°å€¼æ¯”è¾ƒæ¥éªŒè¯ã€‚
- **å¢é‡å¼å¤æ‚åº¦**ï¼šä»å•ä¸ªç®—å­å¼€å§‹ï¼Œé€æ­¥æ„å»ºå®Œæ•´çš„ç¥ç»ç½‘ç»œç»„ä»¶ã€‚
- **å†…å­˜ç®¡ç†ç²¾é€š**ï¼šæ˜¾å¼çš„CUDAå†…å­˜ç®¡ç†å±•ç¤ºäº†å¯¹GPUè®¡ç®—åŸç†çš„æ·±åº¦ç†è§£ã€‚
- **é”™è¯¯é©±åŠ¨å­¦ä¹ **ï¼šå…¨é¢çš„è°ƒè¯•è¿‡ç¨‹æ–‡æ¡£åŒ–ï¼Œä¸ºæœªæ¥å‚è€ƒå’Œå­¦ä¹ æä¾›å®è´µèµ„æ–™ã€‚

**å…³é”®æŠ€æœ¯æˆå°±ï¼š**

- **çº¯GPUæµæ°´çº¿**ï¼š`ffn_cuda` å®ç°åœ¨GPUä¸Šç¼–æ’æ•´ä¸ªè®¡ç®—å›¾ï¼Œæ— éœ€CPU-GPUå¾€è¿”ä¼ è¾“ã€‚
- **å¹¶è¡Œè®¡ç®—ç²¾é€š**ï¼šå®ç°äº†å¤šç§å¹¶è¡Œæ¨¡å¼ï¼š
  - äºŒç»´å¹¶è¡Œ (çŸ©é˜µä¹˜æ³•)
  - ä¸€ç»´å½’çº¦å¹¶è¡Œ (LayerNormå¸¦å…±äº«å†…å­˜)
  - ä¸€ç»´é€å…ƒç´ å¹¶è¡Œ (GELUæ¿€æ´»å‡½æ•°)
- **ç³»ç»Ÿé›†æˆå“è¶Šæ€§**ï¼šæˆåŠŸç¼–æ’å¤šæ ¸å‡½æ•°å·¥ä½œæµï¼Œæ­£ç¡®ç®¡ç†å†…å­˜ç”Ÿå‘½å‘¨æœŸã€‚
- **æ¨¡å—åŒ–æ¶æ„**ï¼šCPUå‚è€ƒå®ç°ä¸GPUä¼˜åŒ–ä¹‹é—´çš„æ¸…æ™°åˆ†ç¦»ã€‚

**å·¥ç¨‹ç»éªŒæ€»ç»“ï¼š**

- **"è°ƒè¯•ç¬¬ä¸€å®šå¾‹"**ï¼šæœ€å…³é”®çš„é”™è¯¯å¾€å¾€éšè—åœ¨æœ€æ„æƒ³ä¸åˆ°çš„åœ°æ–¹ã€‚
- **å†…å­˜å±‚æ¬¡ä¼˜åŒ–**ï¼šç†è§£å¯„å­˜å™¨ vs å…±äº«å†…å­˜ vs å…¨å±€å†…å­˜çš„è®¿é—®æ¨¡å¼ã€‚
- **å¼‚æ­¥æ‰§è¡Œæ„è¯†**ï¼šCPUä¸GPUæ‰§è¡Œä¸Šä¸‹æ–‡ä¹‹é—´çš„æ­£ç¡®åŒæ­¥ã€‚

#### ğŸ—ºï¸ å¼€å‘è·¯çº¿å›¾

**âœ… ç¬¬ä¸€é˜¶æ®µï¼š"å…¨æ ˆç®—å­é”»é€ " - å·²å®Œæˆ**
- [x] æ ¸å¿ƒå¼ é‡æ•°æ®ç»“æ„ä¸CPUå‚è€ƒå®ç°
- [x] æ‰€æœ‰åŸºç¡€ç®—å­çš„GPUæ ¸å‡½æ•°å®ç°
- [x] å¸¦å¤šæ ¸å‡½æ•°ç¼–æ’çš„å®Œæ•´å‰é¦ˆç¥ç»ç½‘ç»œ
- [x] å…¨é¢çš„æµ‹è¯•ä¸éªŒè¯æ¡†æ¶
- [x] æ¨¡å—åŒ–æ¶æ„ä¸ä¸“ä¸šå·¥ç¨‹å®è·µ
- [x] æŠ€æœ¯æ–‡æ¡£åŒ–ä¸çŸ¥è¯†æ€»ç»“

**ğŸš§ ç¬¬äºŒé˜¶æ®µï¼š"é«˜æ€§èƒ½ç³»ç»Ÿé›†æˆ" - å³å°†å¼€å§‹**
- [ ] é«˜çº§GPUå†…å­˜ä¼˜åŒ–æŠ€æœ¯
- [ ] æ ¸å‡½æ•°èåˆä¸è®¡ç®—å›¾ä¼˜åŒ–
- [ ] å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°
- [ ] å®Œæ•´GPT-2 Transformerå—ç»„è£…
- [ ] æ€§èƒ½åˆ†æä¸ç“¶é¢ˆè¯†åˆ«

**ğŸ”® ç¬¬ä¸‰é˜¶æ®µï¼š"ç”Ÿäº§å°±ç»ªæ¨ç†å¼•æ“" - è§„åˆ’ä¸­**
- [ ] é¢„è®­ç»ƒæ£€æŸ¥ç‚¹æ¨¡å‹åŠ è½½ (GPT-2)
- [ ] æ–‡æœ¬æ ‡è®°åŒ–ä¸é¢„å¤„ç†æµæ°´çº¿
- [ ] æ‰¹é‡æ¨ç†ä¼˜åŒ–
- [ ] ç«¯åˆ°ç«¯æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›
- [ ] ä¸ç”Ÿäº§æ¡†æ¶çš„æ€§èƒ½åŸºå‡†å¯¹æ¯”
