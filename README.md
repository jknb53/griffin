```markdown
# ğŸš€ Project Griffin | Griffin é¡¹ç›®

*Read this in other languages: [English](./README.md) | [ä¸­æ–‡](./README.zh-CN.md)* 
*(Note: For a bilingual README in a single file, you can remove the line above and the corresponding Chinese README file.)*

---

### ğŸ‡¬ğŸ‡§ English

A lightweight deep learning inference framework built from scratch in C++ and CUDA. This project is a hands-on implementation following the principles of Andrej Karpathy's "Neural Networks: Zero to Hero" course, with a focus on understanding the first principles of modern deep learning models like GPT-2.

#### ğŸ¯ Project Goals

The primary objective of this project is not to build a production-ready library, but to serve as a rigorous, practice-based learning journey. The key goals are:

-   **Deepen Understanding of Transformers:** Deconstruct high-level concepts like Self-Attention into their fundamental mathematical operators and implement them from scratch.
-   **Master C++/CUDA Programming:** Gain proficiency in C++/CUDA for high-performance computing, including memory management, kernel programming, and orchestrating complex computation flows on the GPU.
-   **Embrace Professional Engineering Practices:** Adhere to a strict workflow using modern CMake for building, Git for version control (following a feature-branching model), and a TDD-like approach by verifying every CUDA implementation against a "golden standard" CPU version.

#### âœ¨ Features Implemented

This project is being built incrementally. The following core components have been successfully implemented and verified:

-   **Core Data Structure:**
    -   [x] A simple `Tensor` struct in C++ for handling multi-dimensional data.

-   **CPU Operators (`cpu_ops.cpp`):**
    -   [x] `matmul_cpu`: Naive matrix multiplication.
    -   [x] `softmax_cpu`: Numerically stable softmax, applied row-wise.
    -   [x] `self_attention`: A complete, verifiable CPU implementation of the self-attention mechanism.

-   **GPU Operators (`kernel.cu`):**
    -   [x] `matmul_cuda`: A pure-GPU matrix multiplication implementation.
    -   [x] `self_attention_cuda_v2`: A high-performance, pure-GPU version of self-attention that orchestrates all computations on the device to eliminate CPU-GPU data roundtrips.
        -   Includes custom kernels: `scale_kernel` and a simplified `softmax_kernel`.

-   **Build & Test System:**
    -   [x] A robust build system configured with CMake to handle C++/CUDA mixed compilation.
    -   [x] A testing framework within `main.cpp` to compare CPU and GPU outputs for correctness.

#### ğŸ› ï¸ How to Build and Run

##### Prerequisites

-   A C++ compiler (g++)
-   NVIDIA CUDA Toolkit (nvcc)
-   CMake (version 3.10+)

##### Build Steps

Clone the repository and run the following commands from the project root directory:

```bash
mkdir build
cd build
cmake ..
make
```

##### Run Tests

The main executable runs a series of tests to verify the correctness of the implemented operators.

```bash
./griffin
```

A `[SUCCESS]` message indicates that the CPU and GPU implementations produce matching results.

#### ğŸ—ºï¸ Future Roadmap (Next Steps)

- [ ] Implement `LayerNorm` Kernel.
- [ ] Optimize `softmax_kernel` with parallel reduction.
- [ ] Optimize `matmul_kernel` with Shared Memory.
- [ ] Implement GELU Activation Kernel.
- [ ] Assemble a full GPT-2 Transformer Block.
- [ ] Build the final GPT-2 Model for inference.

---

### ğŸ‡¨ğŸ‡³ ä¸­æ–‡

ä¸€ä¸ªä»é›¶å¼€å§‹ï¼Œä½¿ç”¨C++å’ŒCUDAæ„å»ºçš„è½»é‡çº§æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶ã€‚æœ¬é¡¹ç›®æ˜¯å¯¹ Andrej Karpathy çš„ "Neural Networks: Zero to Hero" è¯¾ç¨‹ç†å¿µçš„äº²æ‰‹å®è·µï¼Œä¸“æ³¨äºä»ç¬¬ä¸€æ€§åŸç†ç†è§£å¦‚GPT-2ç­‰ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº•å±‚è¿ä½œæœºåˆ¶ã€‚

#### ğŸ¯ é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®çš„æ ¸å¿ƒå¹¶éæ„å»ºä¸€ä¸ªç”Ÿäº§çº§çš„ä»£ç åº“ï¼Œè€Œæ˜¯ä¸€æ¬¡ä¸¥æ ¼çš„ã€åŸºäºå®è·µçš„åˆ»æ„ç»ƒä¹ ä¹‹æ—…ã€‚ä¸»è¦ç›®æ ‡åŒ…æ‹¬ï¼š

- **æ·±åŒ–å¯¹Transformerçš„ç†è§£ï¼š** å°†è‡ªæ³¨æ„åŠ› (Self-Attention) ç­‰é«˜çº§æ¦‚å¿µï¼Œæ‹†è§£ä¸ºå…¶æœ€åŸºç¡€çš„æ•°å­¦ç®—å­ï¼Œå¹¶ä»é›¶å¼€å§‹å®ç°å®ƒä»¬ã€‚
- **æŒæ¡C++/CUDAç¼–ç¨‹ï¼š** ç†Ÿç»ƒè¿ç”¨C++/CUDAè¿›è¡Œé«˜æ€§èƒ½è®¡ç®—ï¼ŒåŒ…æ‹¬å†…å­˜ç®¡ç†ã€æ ¸å‡½æ•°ç¼–ç¨‹ä»¥åŠåœ¨GPUä¸Šç¼–æ’å¤æ‚çš„è®¡ç®—æµã€‚
- **æ‹¥æŠ±ä¸“ä¸šå·¥ç¨‹å®è·µï¼š** éµå¾ªä¸¥è°¨çš„å·¥ä½œæµï¼Œä½¿ç”¨ç°ä»£CMakeæ„å»ºé¡¹ç›®ï¼Œé€šè¿‡Gitè¿›è¡Œç‰ˆæœ¬æ§åˆ¶ï¼ˆéµå¾ªåŠŸèƒ½åˆ†æ”¯æ¨¡å‹ï¼‰ï¼Œå¹¶é‡‡ç”¨ç±»ä¼¼TDDçš„æ–¹æ³•ï¼Œå°†æ¯ä¸€ä¸ªCUDAå®ç°ä¸â€œé»„é‡‘æ ‡å‡†â€çš„CPUç‰ˆæœ¬è¿›è¡Œæ­£ç¡®æ€§éªŒè¯ã€‚

#### âœ¨ å·²å®ç°åŠŸèƒ½

æœ¬é¡¹ç›®é‡‡ç”¨å¢é‡å¼å¼€å‘ã€‚ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶å·²è¢«æˆåŠŸå®ç°å¹¶é€šè¿‡éªŒè¯ï¼š

- **æ ¸å¿ƒæ•°æ®ç»“æ„:**

  - [X] ç”¨äºå¤„ç†å¤šç»´æ•°æ®çš„ç®€æ˜“ `Tensor` C++ ç»“æ„ä½“ã€‚
- **CPU ç®—å­ (`cpu_ops.cpp`):**

  - [X] `matmul_cpu`: æœ´ç´ çš„CPUçŸ©é˜µä¹˜æ³•å®ç°ã€‚
  - [X] `softmax_cpu`: é€è¡Œåº”ç”¨çš„ã€æ•°å€¼ç¨³å®šçš„CPU Softmaxã€‚
  - [X] `self_attention`: ä¸€ä¸ªå®Œæ•´çš„ã€å¯ä½œä¸ºåŸºå‡†çš„CPUè‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°ã€‚
- **GPU ç®—å­ (`kernel.cu`):**

  - [X] `matmul_cuda`: çº¯GPUå®ç°çš„çŸ©é˜µä¹˜æ³•ã€‚
  - [X] `self_attention_cuda_v2`: ä¸€ä¸ªé«˜æ€§èƒ½çš„ã€çº¯GPUç‰ˆæœ¬çš„è‡ªæ³¨æ„åŠ›å®ç°ï¼Œé€šè¿‡å°†æ‰€æœ‰è®¡ç®—ä¿ç•™åœ¨è®¾å¤‡ç«¯ï¼Œæ¶ˆé™¤äº†ä¸å¿…è¦çš„CPU-GPUæ•°æ®å¾€è¿”ã€‚
    - åŒ…å«è‡ªå®šä¹‰æ ¸å‡½æ•°ï¼š`scale_kernel` å’Œä¸€ä¸ªç®€åŒ–çš„ `softmax_kernel`ã€‚
- **æ„å»ºä¸æµ‹è¯•ç³»ç»Ÿ:**

  - [X] ä½¿ç”¨CMakeé…ç½®çš„ã€èƒ½å¤Ÿå¤„ç†C++/CUDAæ··åˆç¼–è¯‘çš„å¥å£®æ„å»ºç³»ç»Ÿã€‚
  - [X] åœ¨ `main.cpp` ä¸­æ­å»ºçš„ã€ç”¨äºå¯¹æ¯”CPUå’ŒGPUè¾“å‡ºä»¥éªŒè¯æ­£ç¡®æ€§çš„æµ‹è¯•æ¡†æ¶ã€‚

#### ğŸ› ï¸ å¦‚ä½•æ„å»ºä¸è¿è¡Œ

##### ç¯å¢ƒè¦æ±‚

- C++ ç¼–è¯‘å™¨ (g++)
- NVIDIA CUDA Toolkit (nvcc)
- CMake (3.10+ç‰ˆæœ¬)

##### æ„å»ºæ­¥éª¤

å…‹éš†æœ¬ä»“åº“ï¼Œå¹¶åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
mkdir build
cd build
cmake ..
make
```

##### è¿è¡Œæµ‹è¯•

ç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶å°†è¿è¡Œä¸€ç³»åˆ—æµ‹è¯•ï¼Œä»¥éªŒè¯å·²å®ç°ç®—å­çš„æ­£ç¡®æ€§ã€‚

```bash
./griffin
```

è¾“å‡º `[SUCCESS]` ä¿¡æ¯ä»£è¡¨CPUå’ŒGPUçš„å®ç°ç»“æœä¸€è‡´ã€‚

#### ğŸ—ºï¸ æœªæ¥è·¯çº¿å›¾ (ä¸‹ä¸€æ­¥è®¡åˆ’)

- [X] å®ç° `LayerNorm` çš„CUDAæ ¸å‡½æ•°ã€‚
- [ ] ä½¿ç”¨å¹¶è¡Œè§„çº¦ (Parallel Reduction) ç®—æ³•ä¼˜åŒ– `softmax_kernel`ã€‚
- [ ] ä½¿ç”¨å…±äº«å†…å­˜ (Shared Memory) ä¼˜åŒ– `matmul_kernel`ã€‚
- [X] å®ç° `GELU` æ¿€æ´»å‡½æ•°çš„CUDAæ ¸å‡½æ•°ã€‚
- [ ] å°†æ‰€æœ‰ç®—å­ç»„è£…æˆä¸€ä¸ªå®Œæ•´çš„GPT-2 Transformeræ¨¡å— (Block)ã€‚
- [ ] æœ€ç»ˆæ„å»ºå‡ºå¯ç”¨äºæ¨ç†çš„å®Œæ•´GPT-2æ¨¡å‹ã€‚
