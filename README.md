```markdown
# ğŸš€ Project Griffin | Griffin é¡¹ç›®

*Bilingual README: Jump to [English](#-english) | [ä¸­æ–‡](#-ä¸­æ–‡)*

---

### ğŸ‡¬ğŸ‡§ English

A lightweight deep learning inference framework built from scratch in C++ and CUDA. This project is a hands-on implementation following the principles of Andrej Karpathy's "Neural Networks: Zero to Hero" course, with a focus on understanding the first principles of modern deep learning models like GPT-2.

#### ğŸ¯ Project Goals

The primary objective of this project is not to build a production-ready library, but to serve as a rigorous, practice-based learning journey. The key goals are:

-   **Deepen Understanding of Transformers:** Deconstruct high-level concepts like Self-Attention into their fundamental mathematical operators and implement them from scratch.
-   **Master C++/CUDA Programming:** Gain proficiency in C++/CUDA for high-performance computing, including memory management, kernel programming, and orchestrating complex computation flows on the GPU.
-   **Embrace Professional Engineering Practices:** Adhere to a strict workflow using modern CMake for building, Git for version control (following a feature-branching model), and a TDD-like approach by verifying every CUDA implementation against a "golden standard" CPU version.

#### âœ¨ Features Implemented

This project is being built incrementally. The following core components have been successfully implemented and verified:

-   **Core Data Structure:**
    -   [x] A simple `Tensor` struct in C++ for handling multi-dimensional data.

-   **CPU Operators (`cpu_ops.cpp`):**
    -   [x] `matmul_cpu`: Naive matrix multiplication.
    -   [x] `softmax_cpu`: Numerically stable softmax, applied row-wise.
    -   [x] `self_attention`: A complete, verifiable CPU implementation of the self-attention mechanism.
    -   [x] `layernorm_cpu`: Layer normalization with configurable gamma and beta parameters.
    -   [x] `gelu_cpu`: Gaussian Error Linear Unit activation function.
    -   [x] `add_bias_cpu`: Bias addition operation with broadcasting support.
    -   [x] `ffn_cpu`: Complete FeedForward Network implementation.

-   **GPU Operators (`kernel.cu`):**
    -   [x] `matmul_cuda`: A pure-GPU matrix multiplication implementation.
    -   [x] `self_attention_cuda_v2`: A high-performance, pure-GPU version of self-attention that orchestrates all computations on the device to eliminate CPU-GPU data roundtrips.
        -   Includes custom kernels: `scale_kernel` and a simplified `softmax_kernel`.
    -   [x] `layernorm_cuda`: GPU-accelerated layer normalization with parallel reduction optimization.
    -   [x] `gelu_cuda`: GPU implementation of GELU activation with custom CUDA kernel.
    -   [x] `add_bias_cuda`: GPU bias addition kernel with efficient memory access patterns.
    -   [x] `ffn_cuda`: Complete GPU pipeline for FeedForward Network with optimized memory management.

-   **Build & Test System:**
    -   [x] A robust build system configured with CMake to handle C++/CUDA mixed compilation.
    -   [x] A testing framework within `main.cpp` to compare CPU and GPU outputs for correctness.

#### ğŸ› ï¸ How to Build and Run

##### Prerequisites

-   A C++ compiler (g++)
-   NVIDIA CUDA Toolkit (nvcc)
-   CMake (version 3.10+)

##### Build Steps

Clone the repository and run the following commands from the project root directory:

```bash
mkdir build
cd build
cmake ..
make
```

##### Run Tests

The main executable runs a comprehensive test to verify the correctness of the FeedForward Network implementation, which includes all core operators working in sequence.

```bash
./griffin
```

A `[SUCCESS]` message indicates that the CPU and GPU implementations of the complete FFN pipeline produce matching results, validating the correctness of:
- Matrix multiplication (`matmul`)
- Bias addition (`add_bias`) 
- GELU activation function (`gelu`)
- The complete FeedForward Network orchestration (`ffn`)

#### ğŸ—ï¸ Architecture Highlights

**Rigorous Development Philosophy:**
- **"CPU Defines Truth"**: Every GPU implementation is validated against its CPU counterpart using strict numerical comparison.
- **Incremental Complexity**: Starting with individual operators and building up to complete neural network components.
- **Memory Management Mastery**: Explicit CUDA memory management demonstrates deep understanding of GPU computing principles.

**Key Technical Achievements:**
- **Pure GPU Pipelines**: The `ffn_cuda` implementation orchestrates an entire computation graph on GPU without CPU-GPU round trips.
- **Parallel Reduction Optimization**: LayerNorm uses shared memory and parallel reduction for efficient variance computation.
- **Modular Design**: Each operator can be tested, verified, and reused independently.

#### ğŸ—ºï¸ Future Roadmap (Next Steps)

- [x] ~~Implement `LayerNorm` Kernel.~~
- [ ] Optimize `softmax_kernel` with parallel reduction.
- [ ] Optimize `matmul_kernel` with Shared Memory and tiled matrix multiplication.
- [x] ~~Implement GELU Activation Kernel.~~
- [ ] Assemble a full GPT-2 Transformer Block (combining Self-Attention, LayerNorm, and FFN).
- [ ] Implement Multi-Head Attention mechanism.
- [ ] Build positional encoding and input embeddings.
- [ ] Create the final GPT-2 Model for inference.
- [ ] Add model loading capabilities (weights from pre-trained checkpoints).
- [ ] Implement text tokenization and generation pipeline.

---

### ğŸ‡¨ğŸ‡³ ä¸­æ–‡

ä¸€ä¸ªä»é›¶å¼€å§‹ï¼Œä½¿ç”¨C++å’ŒCUDAæ„å»ºçš„è½»é‡çº§æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶ã€‚æœ¬é¡¹ç›®æ˜¯å¯¹ Andrej Karpathy çš„ "Neural Networks: Zero to Hero" è¯¾ç¨‹ç†å¿µçš„äº²æ‰‹å®è·µï¼Œä¸“æ³¨äºä»ç¬¬ä¸€æ€§åŸç†ç†è§£å¦‚GPT-2ç­‰ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº•å±‚è¿ä½œæœºåˆ¶ã€‚

#### ğŸ¯ é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®çš„æ ¸å¿ƒå¹¶éæ„å»ºä¸€ä¸ªç”Ÿäº§çº§çš„ä»£ç åº“ï¼Œè€Œæ˜¯ä¸€æ¬¡ä¸¥æ ¼çš„ã€åŸºäºå®è·µçš„åˆ»æ„ç»ƒä¹ ä¹‹æ—…ã€‚ä¸»è¦ç›®æ ‡åŒ…æ‹¬ï¼š

- **æ·±åŒ–å¯¹Transformerçš„ç†è§£ï¼š** å°†è‡ªæ³¨æ„åŠ› (Self-Attention) ç­‰é«˜çº§æ¦‚å¿µï¼Œæ‹†è§£ä¸ºå…¶æœ€åŸºç¡€çš„æ•°å­¦ç®—å­ï¼Œå¹¶ä»é›¶å¼€å§‹å®ç°å®ƒä»¬ã€‚
- **æŒæ¡C++/CUDAç¼–ç¨‹ï¼š** ç†Ÿç»ƒè¿ç”¨C++/CUDAè¿›è¡Œé«˜æ€§èƒ½è®¡ç®—ï¼ŒåŒ…æ‹¬å†…å­˜ç®¡ç†ã€æ ¸å‡½æ•°ç¼–ç¨‹ä»¥åŠåœ¨GPUä¸Šç¼–æ’å¤æ‚çš„è®¡ç®—æµã€‚
- **æ‹¥æŠ±ä¸“ä¸šå·¥ç¨‹å®è·µï¼š** éµå¾ªä¸¥è°¨çš„å·¥ä½œæµï¼Œä½¿ç”¨ç°ä»£CMakeæ„å»ºé¡¹ç›®ï¼Œé€šè¿‡Gitè¿›è¡Œç‰ˆæœ¬æ§åˆ¶ï¼ˆéµå¾ªåŠŸèƒ½åˆ†æ”¯æ¨¡å‹ï¼‰ï¼Œå¹¶é‡‡ç”¨ç±»ä¼¼TDDçš„æ–¹æ³•ï¼Œå°†æ¯ä¸€ä¸ªCUDAå®ç°ä¸â€œé»„é‡‘æ ‡å‡†â€çš„CPUç‰ˆæœ¬è¿›è¡Œæ­£ç¡®æ€§éªŒè¯ã€‚

#### âœ¨ å·²å®ç°åŠŸèƒ½

æœ¬é¡¹ç›®é‡‡ç”¨å¢é‡å¼å¼€å‘ã€‚ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶å·²è¢«æˆåŠŸå®ç°å¹¶é€šè¿‡éªŒè¯ï¼š

- **æ ¸å¿ƒæ•°æ®ç»“æ„:**

  - [X] ç”¨äºå¤„ç†å¤šç»´æ•°æ®çš„ç®€æ˜“ `Tensor` C++ ç»“æ„ä½“ã€‚
- **CPU ç®—å­ (`cpu_ops.cpp`):**

  - [X] `matmul_cpu`: æœ´ç´ çš„CPUçŸ©é˜µä¹˜æ³•å®ç°ã€‚
  - [X] `softmax_cpu`: é€è¡Œåº”ç”¨çš„ã€æ•°å€¼ç¨³å®šçš„CPU Softmaxã€‚
  - [X] `self_attention`: ä¸€ä¸ªå®Œæ•´çš„ã€å¯ä½œä¸ºåŸºå‡†çš„CPUè‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°ã€‚
  - [X] `layernorm_cpu`: å¸¦æœ‰å¯é…ç½®gammaå’Œbetaå‚æ•°çš„å±‚å½’ä¸€åŒ–ã€‚
  - [X] `gelu_cpu`: é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒæ¿€æ´»å‡½æ•°ã€‚
  - [X] `add_bias_cpu`: æ”¯æŒå¹¿æ’­æœºåˆ¶çš„åç½®åŠ æ³•æ“ä½œã€‚
  - [X] `ffn_cpu`: å®Œæ•´çš„å‰é¦ˆç¥ç»ç½‘ç»œå®ç°ã€‚
- **GPU ç®—å­ (`kernel.cu`):**

  - [X] `matmul_cuda`: çº¯GPUå®ç°çš„çŸ©é˜µä¹˜æ³•ã€‚
  - [X] `self_attention_cuda_v2`: ä¸€ä¸ªé«˜æ€§èƒ½çš„ã€çº¯GPUç‰ˆæœ¬çš„è‡ªæ³¨æ„åŠ›å®ç°ï¼Œé€šè¿‡å°†æ‰€æœ‰è®¡ç®—ä¿ç•™åœ¨è®¾å¤‡ç«¯ï¼Œæ¶ˆé™¤äº†ä¸å¿…è¦çš„CPU-GPUæ•°æ®å¾€è¿”ã€‚
    - åŒ…å«è‡ªå®šä¹‰æ ¸å‡½æ•°ï¼š`scale_kernel` å’Œä¸€ä¸ªç®€åŒ–çš„ `softmax_kernel`ã€‚
  - [X] `layernorm_cuda`: å¸¦æœ‰å¹¶è¡Œè§„çº¦ä¼˜åŒ–çš„GPUåŠ é€Ÿå±‚å½’ä¸€åŒ–ã€‚
  - [X] `gelu_cuda`: å¸¦æœ‰è‡ªå®šä¹‰CUDAæ ¸å‡½æ•°çš„GELUæ¿€æ´»å‡½æ•°GPUå®ç°ã€‚
  - [X] `add_bias_cuda`: å…·æœ‰é«˜æ•ˆå†…å­˜è®¿é—®æ¨¡å¼çš„GPUåç½®åŠ æ³•æ ¸å‡½æ•°ã€‚
  - [X] `ffn_cuda`: å…·æœ‰ä¼˜åŒ–å†…å­˜ç®¡ç†çš„å®Œæ•´å‰é¦ˆç¥ç»ç½‘ç»œGPUæµæ°´çº¿ã€‚
- **æ„å»ºä¸æµ‹è¯•ç³»ç»Ÿ:**

  - [X] ä½¿ç”¨CMakeé…ç½®çš„ã€èƒ½å¤Ÿå¤„ç†C++/CUDAæ··åˆç¼–è¯‘çš„å¥å£®æ„å»ºç³»ç»Ÿã€‚
  - [X] åœ¨ `main.cpp` ä¸­æ­å»ºçš„ã€ç”¨äºå¯¹æ¯”CPUå’ŒGPUè¾“å‡ºä»¥éªŒè¯æ­£ç¡®æ€§çš„æµ‹è¯•æ¡†æ¶ã€‚

#### ğŸ› ï¸ å¦‚ä½•æ„å»ºä¸è¿è¡Œ

##### ç¯å¢ƒè¦æ±‚

- C++ ç¼–è¯‘å™¨ (g++)
- NVIDIA CUDA Toolkit (nvcc)
- CMake (3.10+ç‰ˆæœ¬)

##### æ„å»ºæ­¥éª¤

å…‹éš†æœ¬ä»“åº“ï¼Œå¹¶åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
mkdir build
cd build
cmake ..
make
```

##### è¿è¡Œæµ‹è¯•

ç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶å°†è¿è¡Œä¸€ä¸ªç»¼åˆæµ‹è¯•ï¼ŒéªŒè¯å‰é¦ˆç¥ç»ç½‘ç»œå®ç°çš„æ­£ç¡®æ€§ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰æ ¸å¿ƒç®—å­æŒ‰é¡ºåºå·¥ä½œçš„æƒ…å†µã€‚

```bash
./griffin
```

è¾“å‡º `[SUCCESS]` ä¿¡æ¯ä»£è¡¨CPUå’ŒGPUçš„å®Œæ•´FFNæµæ°´çº¿å®ç°ç»“æœä¸€è‡´ï¼ŒéªŒè¯äº†ä»¥ä¸‹ç»„ä»¶çš„æ­£ç¡®æ€§ï¼š
- çŸ©é˜µä¹˜æ³• (`matmul`)
- åç½®åŠ æ³• (`add_bias`)
- GELUæ¿€æ´»å‡½æ•° (`gelu`)
- å®Œæ•´å‰é¦ˆç¥ç»ç½‘ç»œç¼–æ’ (`ffn`)

#### ğŸ—ï¸ æ¶æ„äº®ç‚¹

**ä¸¥è°¨çš„å¼€å‘ç†å¿µï¼š**
- **"CPUå®šä¹‰çœŸç†"**ï¼šæ¯ä¸ªGPUå®ç°éƒ½é€šè¿‡ä¸å…¶CPUå¯¹åº”ç‰ˆæœ¬è¿›è¡Œä¸¥æ ¼çš„æ•°å€¼æ¯”è¾ƒæ¥éªŒè¯ã€‚
- **å¢é‡å¼å¤æ‚åº¦**ï¼šä»å•ä¸ªç®—å­å¼€å§‹ï¼Œé€æ­¥æ„å»ºå®Œæ•´çš„ç¥ç»ç½‘ç»œç»„ä»¶ã€‚
- **å†…å­˜ç®¡ç†ç²¾é€š**ï¼šæ˜¾å¼çš„CUDAå†…å­˜ç®¡ç†å±•ç¤ºäº†å¯¹GPUè®¡ç®—åŸç†çš„æ·±åº¦ç†è§£ã€‚

**å…³é”®æŠ€æœ¯æˆå°±ï¼š**
- **çº¯GPUæµæ°´çº¿**ï¼š`ffn_cuda` å®ç°åœ¨GPUä¸Šç¼–æ’æ•´ä¸ªè®¡ç®—å›¾ï¼Œæ— éœ€CPU-GPUå¾€è¿”ä¼ è¾“ã€‚
- **å¹¶è¡Œè§„çº¦ä¼˜åŒ–**ï¼šLayerNormä½¿ç”¨å…±äº«å†…å­˜å’Œå¹¶è¡Œè§„çº¦è¿›è¡Œé«˜æ•ˆçš„æ–¹å·®è®¡ç®—ã€‚
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¯ä¸ªç®—å­éƒ½å¯ä»¥ç‹¬ç«‹æµ‹è¯•ã€éªŒè¯å’Œé‡ç”¨ã€‚

#### ğŸ—ºï¸ æœªæ¥è·¯çº¿å›¾ (ä¸‹ä¸€æ­¥è®¡åˆ’)

- [X] ~~å®ç° `LayerNorm` çš„CUDAæ ¸å‡½æ•°ã€‚~~
- [ ] ä½¿ç”¨å¹¶è¡Œè§„çº¦ (Parallel Reduction) ç®—æ³•ä¼˜åŒ– `softmax_kernel`ã€‚
- [ ] ä½¿ç”¨å…±äº«å†…å­˜ (Shared Memory) å’Œåˆ†å—çŸ©é˜µä¹˜æ³•ä¼˜åŒ– `matmul_kernel`ã€‚
- [X] ~~å®ç° `GELU` æ¿€æ´»å‡½æ•°çš„CUDAæ ¸å‡½æ•°ã€‚~~
- [ ] å°†æ‰€æœ‰ç®—å­ç»„è£…æˆä¸€ä¸ªå®Œæ•´çš„GPT-2 Transformeræ¨¡å— (Block)ã€‚
- [ ] å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Attention)ã€‚
- [ ] æ„å»ºä½ç½®ç¼–ç å’Œè¾“å…¥åµŒå…¥å±‚ã€‚
- [ ] æœ€ç»ˆæ„å»ºå‡ºå¯ç”¨äºæ¨ç†çš„å®Œæ•´GPT-2æ¨¡å‹ã€‚
- [ ] æ·»åŠ æ¨¡å‹åŠ è½½åŠŸèƒ½ (ä»é¢„è®­ç»ƒæ£€æŸ¥ç‚¹åŠ è½½æƒé‡)ã€‚
- [ ] å®ç°æ–‡æœ¬æ ‡è®°åŒ–å’Œç”Ÿæˆæµæ°´çº¿ã€‚
